{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于sklearn的文本分类---支持向量机SVM(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">本文是文本分类的第三篇，记录使用朴素贝叶斯进行文本分类任务，数据集下载地址:http://thuctc.thunlp.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">文本分类的主要内容如下:\n",
    "    - 1.基于逻辑回归的文本分类\n",
    "    - 2.基于朴素贝叶斯的文本分类\n",
    "    - 3.基于SVM的文本分类\n",
    "    - 4.基于最大熵的文本分类\n",
    "    - 5.使用LDA进行文档降维以及特征选择\n",
    "    - 6.基于多层感知机MLPC的文本分类\n",
    "    - 7.基于卷积神经网络词级别的文本分类以及调参\n",
    "    - 8.基于卷积神经网络的句子级别的文本分类以及调参\n",
    "    - 9.基于Facebook fastText的快速高效文本分类\n",
    "    - 10.基于RNN的文本分类\n",
    "    - 11.基于LSTM的文本分类\n",
    "    - 12.总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 数据预处理\n",
    ">其中使用的训练数据来自清华大学开源的文本分类数据集，具体的数据处理代码见上面的1-基于逻辑回归的文本分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_with_label(corpus):\n",
    "    \"\"\"\n",
    "    将数据划分为训练数据和样本标签\n",
    "    :param corpus: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    input_x = []\n",
    "    input_y = []\n",
    "\n",
    "    tag = []\n",
    "    if os.path.isfile(corpus):\n",
    "        with codecs.open(corpus, 'r') as f:\n",
    "            for line in f:\n",
    "                tag.append(line)\n",
    "                \n",
    "    else:\n",
    "        for docs in corpus:\n",
    "            for doc in docs:\n",
    "                tag.append(doc)\n",
    "    tag = shuffle(tag)\n",
    "    for doc in tag:\n",
    "        index = doc.find(' ')\n",
    "        input_y.append(doc[:index])\n",
    "        input_x.append(doc[index + 1 :])\n",
    "\n",
    "    # 打乱数据，避免在采样的时候出现类别不均衡现象\n",
    "    # datasets = np.column_stack([input_x, input_y])\n",
    "    # np.random.shuffle(datasets)\n",
    "    # input_x = []\n",
    "    # input_y = []\n",
    "    # for i in datasets:\n",
    "    #     input_x.append(i[:-1])\n",
    "    #     input_y.append(i[-1:])\n",
    "    return [input_x, input_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">这个函数返回两个值，其中第一个返回值input_x是样本数据，一共14*1000行，第二个参数input_y和input_x有着相同的行数，每行对应着input_x中新闻样本的类别标签."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">下面将进行特征提取，特征选择的方法有基本的bag-of-words, tf-idf,n-gran等，我们在使用朴素贝叶斯做相关特征选择的时候，进行了相关方法的比较，这里不再重复，特征选择采用TF-IDF,n-gram选择unigram和bigram的条件下进行SVM的文本分类实验，下面是代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(input_x, case='tfidf', n_features=1000):\n",
    "    \"\"\"\n",
    "    特征抽取\n",
    "    :param corpus: \n",
    "    :param case: 不同的特征抽取方法\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # ngram_range=n_gram\n",
    "    return TfidfVectorizer(max_features=n_features).fit_transform(input_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 接下来将进行训练数据和测试数据的切分，现在不进行更好的交叉验证等技术，仅仅简单的以一定的比例划分训练数据和测试数据。使用sklearn中提供的工具，具体代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_to_train_and_test(corpus, indices=0.2, random_state=10, shuffle=True):\n",
    "    \"\"\"\n",
    "    将数据划分为训练数据和测试数据\n",
    "    :param corpus: [input_x]\n",
    "    :param indices: 划分比例\n",
    "    :random_state: 随机种子\n",
    "    :param shuffle: 是否打乱数据\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    input_x, y = corpus\n",
    "\n",
    "    # 切分数据集\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(input_x, y, test_size=indices, random_state=10)\n",
    "    print(\"Vocabulary Size: {:d}\".format(input_x.shape[1]))\n",
    "    print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "    return x_train, x_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 函数返回四个值，分别是训练数据的样本，训练数据的标签，测试数据样本，测试数据真实标签，下面调用朴素贝叶斯进行分类。  \n",
    "\n",
    "> SVM是一种判别式模型，有硬间隔的SVM,软间隔的SVM,还有基于核方法的SVM用于处理非线性问题等，SVM基于支持向量，其分裂结果只和支持向量有关，其在数据中构造一个线性的超平面，来进行数据的区分，因为其能够在小数据的情况下也能训练到一个好的模型在神经网络没有火起来之前，是一个很受欢迎的模型。\n",
    "\n",
    "> 在sklearn中，SVM有三种实现，分别是SVC,NuSVC,LinearSVM ,前者很相似，但是接受的参数和数学推导有区别，LinearSVM是另外一种实现，其是种线性核的SVM，不支持其他核函数。所以在线性分类中，三种方法时一样的，如果要用到诸如RBF核，那么只能选择前两者了。 \n",
    "\n",
    "> 这里主要是进行相关的实验，不在理论上展开太多，下面采用SVC进行文档分类，具体代码如下:  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predicted(train_x, train_y, test_x, test_y, C=1.0, gamma='auto', decision_function_shape='ovr', kernel='linear'):\n",
    "    \"\"\"\n",
    "    训练与预测\n",
    "    :param train_x: \n",
    "    :param train_y: \n",
    "    :param test_x: \n",
    "    :param test_y: \n",
    "    :param C: 松弛变量C\n",
    "    :param gamma: 核系数\n",
    "    :param decision_function_shape: 多分类的决策，ovr（one-vs-rest）default  ovo（one-vs-one） \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    clf = SVC(C=C, gamma=gamma, decision_function_shape=decision_function_shape, kernel=kernel).fit(train_x, train_y)\n",
    "    predicted = clf.predict(test_x)\n",
    "    print(metrics.classification_report(test_y, predicted))\n",
    "    print('accuracy_score: %0.5fs' %(metrics.accuracy_score(test_y, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 上面函数调用SVC()，C,gamma,decision_function_shape三个参数，其中前两个是SVM调参的主要参数，其的值影响了SVM分类器的效果，我们将在后面进行具体的调参工作，最后一个参数是在进行多分类的时候用到的方法， 其有两个可选值:ovo和ovr，ovo表示的是one-vs-one，表示每个二分类只拿两个类别的数据来训练，N个类别训练下来一共要训练x次，ovr表示one-vs-rest,表示每次二分类训练器拿一个类别和其与类别一起训练二分类，N个类别一共需要训练N个分类器，但是前者每次训练的数据量较小，后者虽然训练次数少，但是每次训练的数据量较大，具体选择哪一个，需要根据需要训练的数据量和训练机器的内存等因素来决定\n",
    "\n",
    "> 下面进行文本分类的具体流程，将进行实际的代码运行阶段了。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. 加载语料\n",
    "corpus = split_data_with_label('thu_data_2000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tC=1.0,gama='auto'的SVM文本分类\t\t\n",
      "Vocabulary Size: 1000\n",
      "Train/Dev split: 22400/5600\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "__label__体育       0.93      0.94      0.94       422\n",
      "__label__娱乐       0.86      0.88      0.87       377\n",
      "__label__家居       0.79      0.87      0.83       391\n",
      "__label__彩票       0.99      0.94      0.96       392\n",
      "__label__房产       0.94      0.92      0.93       395\n",
      "__label__教育       0.94      0.92      0.93       391\n",
      "__label__时尚       0.85      0.84      0.85       395\n",
      "__label__时政       0.82      0.86      0.84       418\n",
      "__label__星座       0.96      0.95      0.95       391\n",
      "__label__游戏       0.94      0.88      0.91       420\n",
      "__label__社会       0.81      0.85      0.83       388\n",
      "__label__科技       0.82      0.82      0.82       415\n",
      "__label__股票       0.83      0.88      0.85       412\n",
      "__label__财经       0.92      0.83      0.87       393\n",
      "\n",
      "avg / total       0.89      0.88      0.88      5600\n",
      "\n",
      "accuracy_score: 0.88357s\n",
      "time uesed: 126.9834s\n"
     ]
    }
   ],
   "source": [
    "# 4. 训练以及测试\n",
    "t0 = time()\n",
    "print('\\t\\tC=1.0,gama=\\'auto\\'的SVM文本分类\\t\\t')\n",
    "input_x, y = corpus\n",
    "# 2. 特征选择\n",
    "input_x = feature_extractor(input_x, 'tfidf')\n",
    "# 3.切分训练数据和测试数据\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test([input_x, y])\n",
    "fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 必须说明的是，和前几篇分类比起来，我在特征选择哪里选择了n_feature=1000,以此来降低维度，因为在高维情况下，SVM运行的非常的。。非常的慢。。。因为这个特征的维度已经上升到了500W。\n",
    "> 而且使用RBF核的时候，不仅训练慢，效果也巨不好？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 特征降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 从上面的实验可以看出，特征维度为500W维，这种维度的数据，称为维度灾难，应该不为过了，可以发现在训练的过程中非常的忙。自然而然的就应该想到现在要进行降维了。比较常见的降维方法有PCA降维，当然了，在文本中，降维方法很多很多，本文将会使用LDA方法，训练出文档主题矩阵作为文档的表示，下面将会选用后者。了解LDA请大家去参考轮问吧。下面使用两种方法，进行文档降维："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 PCA降维\n",
    "\n",
    "> 必须说明的是，sklearn 的PCA是不支持稀疏矩阵的输入的，文档说需要使用TruncatedSVD,也就是使用奇异值分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_usePCA(input_x, n_components=1000):\n",
    "    tf_vect = TfidfVectorizer().fit_transform(input_x)\n",
    "    #pca = PCA(n_components=n_components)\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    return svd.fit_transform(tf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t使用PCA降维的SVM文本分类\t\t\n",
      "Vocabulary Size: 1000\n",
      "Train/Dev split: 22411/5603\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "       _体育_       0.95      0.96      0.95       412\n",
      "       _娱乐_       0.85      0.93      0.89       393\n",
      "       _家居_       0.86      0.94      0.90       399\n",
      "       _彩票_       0.99      0.95      0.97       399\n",
      "       _房产_       0.96      0.91      0.93       386\n",
      "       _教育_       0.94      0.89      0.91       422\n",
      "       _时尚_       0.94      0.91      0.93       387\n",
      "       _时政_       0.83      0.91      0.87       372\n",
      "       _星座_       0.95      0.96      0.96       414\n",
      "       _游戏_       0.98      0.90      0.94       410\n",
      "       _社会_       0.83      0.90      0.87       399\n",
      "       _科技_       0.88      0.79      0.83       393\n",
      "       _股票_       0.87      0.89      0.88       398\n",
      "       _财经_       0.94      0.90      0.92       419\n",
      "\n",
      "avg / total       0.91      0.91      0.91      5603\n",
      "\n",
      "accuracy_score: 0.91058s\n",
      "time uesed: 1169.0307s\n"
     ]
    }
   ],
   "source": [
    "# 4. 训练以及测试\n",
    "t0 = time()\n",
    "print('\\t\\t使用SVD降维的SVM文本分类\\t\\t')\n",
    "input_x, y = corpus\n",
    "# 2. 特征选择以及降维\n",
    "input_x = feature_select_usePCA(input_x)\n",
    "# 3.切分训练数据和测试数据\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test([input_x, y])\n",
    "fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 可以看到维度下降到了1000维，比之前直接使用最大词频的1000分类性能有所提升,约提升了2%，但是在特征降维的时候时间消耗比较大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 LDA降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LDA的目标是求解出两个隐含变量：theta:文档主题矩阵，表示文档的主题分布，phi:主题词分布，表示单词在主题下的分布，通过sklearn 中的LatentDirichletAlloc，通过fit_transform()函数，返回[n_samples, new_features_x]，其new_features_x的维度是确定的主题个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_select_useLDA(input_x, n_components=100):\n",
    "    tf_vect = TfidfVectorizer().fit_transform(input_x)\n",
    "    #pca = PCA(n_components=n_compLatentDirichletAllocationents)\n",
    "    lda = LatentDirichletAllocation(n_components=n_components, learning_method='batch', n_jobs=-1)\n",
    "    return lda.fit_transform(tf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t使用LDA降维的SVM文本分类\t\t\n"
     ]
    }
   ],
   "source": [
    "# 4. 训练以及测试\n",
    "t0 = time()\n",
    "print('\\t\\t使用LDA降维的SVM文本分类\\t\\t')\n",
    "input_x, y = corpus\n",
    "# 2. 特征选择以及降维\n",
    "input_x = feature_select_useLDA(input_x, 1000)\n",
    "# 3.切分训练数据和测试数据\n",
    "train_x, test_x, train_y, test_y = split_data_to_train_and_test([input_x, y])\n",
    "fit_and_predicted(train_x, train_y, test_x, test_y)\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 使用LDA降维的LDA参数调优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LDA需要传入的参数主要有主题个数，训练过程是使用bath还是online，狄里克莱先验，最大迭代次数，下面我们进行LDA参数调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_LDA(corpus, param_grid, cv=5):\n",
    "    input_x, y = corpus\n",
    "    \n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    clf = LatentDirichletAllocation(max_iter=500, n_job=-1)\n",
    "    grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    tf_vect = TfidfVectorizer().fit_transform(input_x)\n",
    "    scores = grid.fit(tf_vect, y)\n",
    "    \n",
    "    print('parameters:')\n",
    "    best_parameters = grid.best_estimator_.get_params()\n",
    "    for param_name in sorted(best_parameters):\n",
    "        print('\\t%s: %r' %(param_name, best_parameters[param_name]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4. 训练以及测试\n",
    "t0 = time()\n",
    "print('\\t\\tLDA调参\\t\\t')\n",
    "\n",
    "n_components = [14, 20, 30, 50, 100]\n",
    "learning_method = ['online', 'batch']\n",
    "learning_decay = [0.1, 0.2, 0.5, 1.0, 2.0]\n",
    "param_grid = dict(n_components=n_components, learning_method=learning_method, learning_decay=learning_decay)\n",
    "\n",
    "fit_LDA(corpus, param_grid, )\n",
    "print('time uesed: %0.4fs' %(time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 通过两种降维方法可以看出，使用！！！\n",
    "\n",
    "> 最后我们使用基于SVD的降维方法，采用线性核，进行SVM文本分类的交叉验证和调参工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用交叉验证\n",
    "> 上面的实验中，我们只是简单的选取20%的数据作为测试集和80%的数据作为训练集，这样做是存在偶然性结构的， 即可能划分数据集不能表示真实的数据分布，导致模型训练参数的泛化性不好，采用交叉验证可以避免数据集划分导致的问题，下面，就进行该实验，实验在上一步的基础上使用TF-IDF和unigram,bigram和trigram来进行特征选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_with_CV(corpus, cv=5, alpha=1, fit_prior=True):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    input_x, y = corpus\n",
    "#     scoring = {'prec_macro': 'precision_macro',\n",
    "#                'rec_micro': make_scorer(recall_score, average='macro')}\n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    clf = SVC(alpha=alpha, fit_prior=fit_prior)\n",
    "    scores = cross_validate(clf, input_x, y, scoring=scoring,\n",
    "                            cv=cv, return_train_score=True)\n",
    "    sorted(scores.keys()) \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x, y = corpus\n",
    "# 2. 特征选择\n",
    "input_x = feature_extractor(input_x, 'tfidf')\n",
    "scores = train_and_test_with_CV([input_x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 0.69856882,  0.6891861 ,  0.68457079,  0.68122745,  0.68401599]),\n",
       " 'score_time': array([ 0.24055672,  0.25055385,  0.24642444,  0.24583435,  0.25062966]),\n",
       " 'test_f1_macro': array([ 0.93190598,  0.93358814,  0.92900074,  0.93620104,  0.93139325]),\n",
       " 'test_precision_macro': array([ 0.93411186,  0.93509947,  0.93082131,  0.93790787,  0.93312355]),\n",
       " 'test_recall_macro': array([ 0.93178571,  0.93357143,  0.92892857,  0.93607143,  0.93142857]),\n",
       " 'train_f1_macro': array([ 0.95534592,  0.95516529,  0.95665886,  0.95573948,  0.95629695]),\n",
       " 'train_precision_macro': array([ 0.95629235,  0.95618146,  0.95767379,  0.9566414 ,  0.95725075]),\n",
       " 'train_recall_macro': array([ 0.95526786,  0.95508929,  0.95660714,  0.95571429,  0.95625   ])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">交叉验证的K=10的时候"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = train_and_test_with_CV([input_x, y],cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([ 0.86708903,  0.85473442,  0.85248995,  0.8252821 ,  0.93414092,\n",
       "         1.118325  ,  1.41779876,  1.2739253 ,  1.98447776,  1.11306906]),\n",
       " 'score_time': array([ 0.16501474,  0.16674805,  0.17412877,  0.15616584,  0.14272356,\n",
       "         0.21593046,  0.44325757,  0.30753231,  0.19881511,  0.20148587]),\n",
       " 'test_f1_macro': array([ 0.93355446,  0.93725727,  0.9367952 ,  0.93744957,  0.9319552 ,\n",
       "         0.93147271,  0.94146465,  0.93213457,  0.93504439,  0.93282066]),\n",
       " 'test_precision_macro': array([ 0.93583195,  0.93947505,  0.93829325,  0.93885285,  0.9343669 ,\n",
       "         0.93272889,  0.94303357,  0.93393932,  0.93704557,  0.93441742]),\n",
       " 'test_recall_macro': array([ 0.93357143,  0.93714286,  0.93678571,  0.9375    ,  0.93178571,\n",
       "         0.93142857,  0.94142857,  0.93214286,  0.935     ,  0.93285714]),\n",
       " 'train_f1_macro': array([ 0.9565462 ,  0.95530877,  0.95550728,  0.95550059,  0.9569892 ,\n",
       "         0.95626531,  0.95577014,  0.95573623,  0.95608533,  0.95600703]),\n",
       " 'train_precision_macro': array([ 0.95739641,  0.95630651,  0.95651889,  0.95645792,  0.95790256,\n",
       "         0.95727949,  0.95675378,  0.95657705,  0.9570335 ,  0.95699902]),\n",
       " 'train_recall_macro': array([ 0.95650794,  0.9552381 ,  0.95543651,  0.95543651,  0.95694444,\n",
       "         0.95619048,  0.95571429,  0.95571429,  0.95603175,  0.95595238])}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 寻找最好的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 朴素贝叶斯的参数比偶较少，根据sklearn的文档可以看出，其参数主要是平滑项参数alpha、是否需要依靠样本去学习类别先验fit_prior和给定类别先验class_prio的给定.\n",
    "\n",
    "> 下面对这些参数做相关实验。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "def train_and_predicted_with_graid(corpus, cv, param_grid):\n",
    "    input_x, y = corpus\n",
    "    \n",
    "    scoring = ['precision_macro', 'recall_macro', 'f1_macro']\n",
    "    clf = MultinomialNB()\n",
    "    grid = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    scores = grid.fit(input_x, y)\n",
    "    \n",
    "    print('parameters:')\n",
    "    best_parameters = grid.best_estimator_.get_params()\n",
    "    for param_name in sorted(best_parameters):\n",
    "        print('\\t%s: %r' %(param_name, best_parameters[param_name]))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_prior': [True, False], 'alpha': [0, 1, 2, 4, 10]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters:\n",
      "\talpha: 0\n",
      "\tclass_prior: None\n",
      "\tfit_prior: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    }
   ],
   "source": [
    "k_alpha = [0, 1,2,4,10]\n",
    "fit_prior= [True, False]\n",
    "param_grid = dict(alpha=k_alpha, fit_prior=fit_prior)\n",
    "print(param_grid)\n",
    "scores = train_and_predicted_with_graid([input_x, y], 5, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_recall_macro': array([ 0.93357143,  0.93714286,  0.93678571,  0.9375    ,  0.93178571,\n",
      "        0.93142857,  0.94142857,  0.93214286,  0.935     ,  0.93285714]), 'test_precision_macro': array([ 0.93583195,  0.93947505,  0.93829325,  0.93885285,  0.9343669 ,\n",
      "        0.93272889,  0.94303357,  0.93393932,  0.93704557,  0.93441742]), 'train_recall_macro': array([ 0.95650794,  0.9552381 ,  0.95543651,  0.95543651,  0.95694444,\n",
      "        0.95619048,  0.95571429,  0.95571429,  0.95603175,  0.95595238]), 'fit_time': array([ 0.86708903,  0.85473442,  0.85248995,  0.8252821 ,  0.93414092,\n",
      "        1.118325  ,  1.41779876,  1.2739253 ,  1.98447776,  1.11306906]), 'train_f1_macro': array([ 0.9565462 ,  0.95530877,  0.95550728,  0.95550059,  0.9569892 ,\n",
      "        0.95626531,  0.95577014,  0.95573623,  0.95608533,  0.95600703]), 'test_f1_macro': array([ 0.93355446,  0.93725727,  0.9367952 ,  0.93744957,  0.9319552 ,\n",
      "        0.93147271,  0.94146465,  0.93213457,  0.93504439,  0.93282066]), 'train_precision_macro': array([ 0.95739641,  0.95630651,  0.95651889,  0.95645792,  0.95790256,\n",
      "        0.95727949,  0.95675378,  0.95657705,  0.9570335 ,  0.95699902]), 'score_time': array([ 0.16501474,  0.16674805,  0.17412877,  0.15616584,  0.14272356,\n",
      "        0.21593046,  0.44325757,  0.30753231,  0.19881511,  0.20148587])}\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 使用最佳参数进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test_recall_macro': array([ 0.98714286,  0.98607143,  0.98642857,  0.98607143,  0.98178571,\n",
      "        0.98464286,  0.98535714,  0.98214286,  0.97714286,  0.98392857]), 'test_precision_macro': array([ 0.98735865,  0.98622353,  0.98659011,  0.98618267,  0.98201582,\n",
      "        0.9849389 ,  0.98558907,  0.9825292 ,  0.97748478,  0.9840529 ]), 'train_recall_macro': array([ 0.99809524,  0.99781746,  0.99785714,  0.99789683,  0.9975    ,\n",
      "        0.9975    ,  0.9977381 ,  0.99801587,  0.99785714,  0.99781746]), 'fit_time': array([ 0.81418514,  0.80592179,  0.81234241,  0.79919314,  0.8071866 ,\n",
      "        0.79807925,  0.79640722,  0.77489328,  0.80264211,  0.7880013 ]), 'train_f1_macro': array([ 0.9980958 ,  0.99781816,  0.99785777,  0.99789754,  0.99750104,\n",
      "        0.99750157,  0.99773862,  0.99801643,  0.99785776,  0.99781855]), 'test_f1_macro': array([ 0.98716361,  0.98606902,  0.98643996,  0.98603623,  0.98180131,\n",
      "        0.98463883,  0.98534904,  0.98216567,  0.9771135 ,  0.98393848]), 'train_precision_macro': array([ 0.99810511,  0.99782832,  0.99787039,  0.99790794,  0.99751509,\n",
      "        0.99751977,  0.99775029,  0.99802555,  0.99786749,  0.99783064]), 'score_time': array([ 0.17398071,  0.14726663,  0.16023517,  0.14582086,  0.17289758,\n",
      "        0.14939237,  0.15340734,  0.14639425,  0.15420914,  0.15345049])}\n"
     ]
    }
   ],
   "source": [
    "scores = train_and_test_with_CV([input_x, y], cv=10, alpha=0)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">可以看到，训练得到的结果相对于在没有进行最优参数调整的时候提高了约5%，效果是明显的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">本文记录了使用sklearn，采用朴素贝叶斯进行文本分类任务，在使用简单的bag-of-word,tf-idf 作为参数选择，为了增加特征，保留句子中的部分语义信息，\n",
    "，我们还进行了n-gram操作，在特征选择阶段，我们发现，使用tf-idf的特征表示方法比简单的词袋模型要好，添加了n-gram特征后，效果也有一定的提升；\n",
    "\n",
    "> 在选取好了特征后，我们对数据集进行交叉验证，发现cv=10相对cv=5的时候有细微的提升，但是效果不明显，说明本数据集在cv=5的时候已经够用了，不需要再继续使用CV=10增加计算量；\n",
    "\n",
    "> 最后，我们进行了最佳参数的寻找，由于naive bayes 分类器的参数较少，调参起来相对简单，在选用了最佳的参数后，我们得出了相对最优的结果，在测试集上P,R,F值几乎都达到了98%以上。  \n",
    "但是分析我们的最佳参数，其中平滑项参数我们选取的是0，在模型中说明是不需要进行数据的平滑处理，但是经验而言，当数据变大，在开放的数据中，平滑项是必不可少的，此处的0，只是作为最有参数选寻找的个例，不应该用作一般性结论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}