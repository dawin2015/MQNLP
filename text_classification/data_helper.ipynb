{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基于THU语料库的文本分类实验的数据处理util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib import learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#category = ['星座', '股票', '房产', '时尚', '体育', '社会', '家居', '游戏', '彩票', '科技', '教育', '时政', '娱乐', '财经']\n",
    "category = ['体育', '股票', '科技']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_and_label(corpus):\n",
    "    \"\"\"\n",
    "    将数据划分为训练数据和样本标签\n",
    "    :param corpus: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    input_x = []\n",
    "    input_y = []\n",
    "\n",
    "    tag = []\n",
    "    if os.path.isfile(corpus):\n",
    "        with codecs.open(corpus, 'r') as f:\n",
    "            for line in f:\n",
    "                tag.append(re.sub('[\\xa0\\n\\r\\t]+', '' , line))\n",
    "                \n",
    "    else:\n",
    "        for docs in corpus:\n",
    "            for doc in docs:\n",
    "                tag.append(doc)\n",
    "    tag = shuffle(tag)\n",
    "    for doc in tag:\n",
    "        index = doc.find(' ')\n",
    "        tag= doc[:index]\n",
    "        tag = re.sub('__label__', '', tag)\n",
    "        try:\n",
    "            i = category.index(tag)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        input_y.append(i)\n",
    "        \n",
    "        input_x.append(doc[index + 1 :])\n",
    "    \n",
    "    return [input_x, input_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pad sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(input_x, maxlen):\n",
    "    \"\"\"\n",
    "    对数据进行padding,短的进行填充，长的进行截取\n",
    "    :param input_x: \n",
    "    :return: 转化为index的语料库以及word:id的矩阵\n",
    "    \"\"\"\n",
    "\n",
    "    #  keras method for corpus preprocess...\n",
    "    # tokenizer = Tokenizer(num_words=num_words)\n",
    "    # tokenizer.fit_on_texts(input_x)\n",
    "    # # 将原始的词语转化为index形式\n",
    "    # sequences = np.array(tokenizer.texts_to_sequences(input_x))\n",
    "    #\n",
    "    # # for maxlen and encode text to index less using padding\n",
    "    # max_len = max([len(x.split(' ')) for x in input_x])\n",
    "    # if maxlen is None:\n",
    "    #     maxlen = max_len\n",
    "    # maxlen = min(max_len, maxlen)\n",
    "    # sequences = sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "    # return sequence, tokenizer.word_index\n",
    "\n",
    "    # tf method\n",
    "\n",
    "    max_len = max([len(x.split(' ')) for x in input_x])\n",
    "    if maxlen is None:\n",
    "        maxlen = max_len\n",
    "    maxlen = min(max_len, maxlen)\n",
    "    vocab_process = learn.preprocessing.VocabularyProcessor(max_document_length=maxlen)\n",
    "    input_x = np.array(list(vocab_process.fit_transform(input_x)))\n",
    "    return input_x, vocab_process.vocabulary_._mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one-hot for category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_one_hot(targets, nb_classes):\n",
    "    \"\"\"\n",
    "    标签进行one-hot 处理\n",
    "    :param targets: 一维的类别列表,类别标签从0开始\n",
    "    :param nb_classes: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    return np.eye(nb_classes)[np.array(targets).reshape(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据，得到处理好的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, maxlen):\n",
    "    \"\"\"\n",
    "    加载数据\n",
    "    :param file_path: \n",
    "    :param num_words: \n",
    "    :param maxlen: \n",
    "    :return: index and padding and numpy input_x, one-hot input_y, word-index mapping \n",
    "    \"\"\"\n",
    "    input_x, input_y = split_data_and_label(file_path)\n",
    "    input_x, words_index = pad_sequence(input_x, maxlen)\n",
    "\n",
    "    label_ = set()\n",
    "    [label_.add(y) for y in input_y]\n",
    "    nb_class = len(label_)\n",
    "    input_y = label_one_hot(input_y, nb_class)\n",
    "    return input_x, input_y, words_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    针对训练数据，组合batch iter\n",
    "    :param data:\n",
    "    :param batch_size: the size of each batch\n",
    "    :param num_epochs: total of epochs\n",
    "    :param shuffle: 是否需要打乱数据\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    # 样本数量\n",
    "    data_size = len(data)\n",
    "    # 根据batch size 计算一个epoch中的batch 数量\n",
    "    num_batches_per_epoch = int((len(data) - 1) / batch_size) + 1\n",
    "    # generates iter for dataset.\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load data from pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_pkl(file_path):\n",
    "    \"\"\"\n",
    "    从制作好的文件中读取测试集和训练集，这样可以避免在不同的实验中，shuffle到不同的train/dev数据\n",
    "    :param file_path: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError('FILE NOT FOND IN %s', file_path)\n",
    "    with open(file_path, 'rb') as fd:\n",
    "        input_x, input_y, word_index = pickle.load(fd)\n",
    "    return input_x, input_y, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    resources = r'C:\\workspace\\python\\MQNLP\\resources'\n",
    "    with\n",
    "    input_x, input_y, word_index = load_data_pkl(os.path.join(resources, 'thu_data_3class_3k.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 400)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
