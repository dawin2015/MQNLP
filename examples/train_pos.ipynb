{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoding=utf8\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import pickle\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.sequencelabeling.model import Model\n",
    "from src.sequencelabeling.loader import load_sentences, update_tag_scheme\n",
    "from src.sequencelabeling.loader import char_mapping, tag_mapping\n",
    "from src.sequencelabeling.loader import augment_with_pretrained, prepare_dataset\n",
    "from src.sequencelabeling.utils import get_logger, make_path, clean, create_model, save_model\n",
    "from src.sequencelabeling.utils import print_config, save_config, load_config, test_ner\n",
    "from src.sequencelabeling.data_utils import load_word2vec, input_from_line, BatchManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "flags.DEFINE_boolean(\"train\",       True,      \"Whether train the model\")\n",
    "# configurations for the model\n",
    "flags.DEFINE_integer(\"seg_dim\",     0,         \"Embedding size for segmentation, 0 if not used\")\n",
    "flags.DEFINE_integer(\"char_dim\",    100,        \"Embedding size for characters\")\n",
    "flags.DEFINE_integer(\"lstm_dim\",    100,        \"Num of hidden units in LSTM, or num of filters in IDCNN\")\n",
    "flags.DEFINE_string(\"tag_schema\",   \"iobes\",    \"tagging schema iobes or iob\")\n",
    "flags.DEFINE_integer(\"num_segs\",    4,        \"Number of sges\")\n",
    "\n",
    "# configurations for training\n",
    "flags.DEFINE_float(\"clip\",          5,          \"Gradient clip\")\n",
    "flags.DEFINE_float(\"dropout\",       0.5,        \"Dropout rate\")\n",
    "flags.DEFINE_float(\"batch_size\",    20,         \"batch size\")\n",
    "flags.DEFINE_float(\"lr\",            0.001,      \"Initial learning rate\")\n",
    "flags.DEFINE_string(\"optimizer\",    \"adam\",     \"Optimizer for training\")\n",
    "flags.DEFINE_boolean(\"pre_emb\",     True,       \"Wither use pre-trained embedding\")\n",
    "flags.DEFINE_boolean(\"zeros\",       False,      \"Wither replace digits with zero\")\n",
    "flags.DEFINE_boolean(\"lower\",       True,       \"Wither lower case\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_epoch\",   100,        \"maximum training epochs\")\n",
    "flags.DEFINE_integer(\"steps_check\", 100,        \"steps per checkpoint\")\n",
    "flags.DEFINE_string(\"ckpt_path\",    r\"../models/pos\",      \"Path to save model\")\n",
    "flags.DEFINE_string(\"summary_path\", \"summary\",      \"Path to store summaries\")\n",
    "flags.DEFINE_string(\"log_file\",     r\"../models/pos/train.log\",    \"File for log\")\n",
    "flags.DEFINE_string(\"map_file\",     r\"../models/pos/maps.pkl\",     \"file for maps\")\n",
    "flags.DEFINE_string(\"vocab_file\",   r\"../models/pos/vocab.json\",   \"File for vocab\")\n",
    "flags.DEFINE_string(\"config_file\",  r\"../models/pos/config_file\",  \"File for config\")\n",
    "flags.DEFINE_string(\"script\",       r\"../../dataset/pos/conlleval\",    \"evaluation script\")\n",
    "flags.DEFINE_string(\"result_path\",  r\"../models/pos/result\",       \"Path for results\")\n",
    "\n",
    "flags.DEFINE_string(\"emb_file\", r'../../dataset/wiki_100.utf8',  \"Path for pre_trained embedding\")\n",
    "flags.DEFINE_string(\"train_file\", r'../../dataset/pos/pos.train',  \"Path for train data\")\n",
    "flags.DEFINE_string(\"dev_file\", r'../../dataset/pos/pos.dev',    \"Path for dev data\")\n",
    "flags.DEFINE_string(\"test_file\", r'../../dataset/pos/pos.test',   \"Path for test data\")\n",
    "\n",
    "#flags.DEFINE_string(\"model_type\", \"idcnn\", \"Model type, can be idcnn or bilstm\")\n",
    "flags.DEFINE_string(\"model_type\", \"bilstm\", \"Model type, can be idcnn or bilstm\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "assert FLAGS.clip < 5.1, \"gradient clip should't be too much\"\n",
    "assert 0 <= FLAGS.dropout < 1, \"dropout rate between 0 and 1\"\n",
    "assert FLAGS.lr > 0, \"learning rate must larger than zero\"\n",
    "assert FLAGS.optimizer in [\"adam\", \"sgd\", \"adagrad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config for the model\n",
    "def config_model(char_to_id, tag_to_id):\n",
    "    config = OrderedDict()\n",
    "    config[\"model_type\"] = FLAGS.model_type\n",
    "    config[\"num_chars\"] = len(char_to_id)\n",
    "    config[\"char_dim\"] = FLAGS.char_dim\n",
    "    config[\"num_tags\"] = len(tag_to_id)\n",
    "    config[\"seg_dim\"] = FLAGS.seg_dim\n",
    "    config[\"num_segs\"] = FLAGS.num_segs\n",
    "    config[\"lstm_dim\"] = FLAGS.lstm_dim\n",
    "    config[\"batch_size\"] = FLAGS.batch_size\n",
    "\n",
    "    config[\"emb_file\"] = FLAGS.emb_file\n",
    "    config[\"clip\"] = FLAGS.clip\n",
    "    config[\"dropout_keep\"] = 1.0 - FLAGS.dropout\n",
    "    config[\"optimizer\"] = FLAGS.optimizer\n",
    "    config[\"lr\"] = FLAGS.lr\n",
    "    config[\"tag_schema\"] = FLAGS.tag_schema\n",
    "    config[\"pre_emb\"] = FLAGS.pre_emb\n",
    "    config[\"zeros\"] = FLAGS.zeros\n",
    "    config[\"lower\"] = FLAGS.lower\n",
    "    return config\n",
    "\n",
    "\n",
    "def evaluate(sess, model, name, data, id_to_tag, logger):\n",
    "    logger.info(\"evaluate:{}\".format(name))\n",
    "    ner_results = model.evaluate(sess, data, id_to_tag)\n",
    "    eval_lines = test_ner(ner_results, FLAGS.result_path)\n",
    "    for line in eval_lines:\n",
    "        logger.info(line)\n",
    "    # 获取F1值\n",
    "    f1 = float(eval_lines[1].strip().split()[-1])\n",
    "\n",
    "    # 输出最好结果\n",
    "    if name == \"dev\":\n",
    "        best_test_f1 = model.best_dev_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            tf.assign(model.best_dev_f1, f1).eval()\n",
    "            logger.info(\"new best dev f1 score:{:>.3f}\".format(f1))\n",
    "        return f1 > best_test_f1\n",
    "    elif name == \"test\":\n",
    "        best_test_f1 = model.best_test_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            tf.assign(model.best_test_f1, f1).eval()\n",
    "            logger.info(\"new best test f1 score:{:>.3f}\".format(f1))\n",
    "        return f1 > best_test_f1\n",
    "\n",
    "\n",
    "def train():\n",
    "    # load data sets\n",
    "    train_sentences = load_sentences(FLAGS.train_file, FLAGS.lower, FLAGS.zeros)\n",
    "    dev_sentences = load_sentences(FLAGS.dev_file, FLAGS.lower, FLAGS.zeros)\n",
    "    test_sentences = load_sentences(FLAGS.test_file, FLAGS.lower, FLAGS.zeros)\n",
    "\n",
    "    # Use selected tagging scheme (IOB / IOBES)\n",
    "    if FLAGS.tag_schema == 'iob':\n",
    "        update_tag_scheme(train_sentences, FLAGS.tag_schema)\n",
    "        update_tag_scheme(test_sentences, FLAGS.tag_schema)\n",
    "\n",
    "    # create maps if not exist  创建index-term 映射表，如果存在则加载，否则创建\n",
    "    if not os.path.isfile(FLAGS.map_file):\n",
    "        # create dictionary for word\n",
    "        if FLAGS.pre_emb:\n",
    "            dico_chars_train = char_mapping(train_sentences + dev_sentences, FLAGS.lower)[0]\n",
    "            dico_chars, char_to_id, id_to_char = augment_with_pretrained(\n",
    "                dico_chars_train.copy(),\n",
    "                FLAGS.emb_file,\n",
    "                list(itertools.chain.from_iterable(\n",
    "                    [[w[0] for w in s] for s in test_sentences])\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            _c, char_to_id, id_to_char = char_mapping(train_sentences, FLAGS.lower)\n",
    "\n",
    "        # Create a dictionary and a mapping for tags\n",
    "        _t, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "        with open(FLAGS.map_file, \"wb\") as f:\n",
    "            pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], f)\n",
    "    else:\n",
    "        with open(FLAGS.map_file, \"rb\") as f:\n",
    "            char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
    "\n",
    "    # prepare data, get a collection of list containing index\n",
    "    train_data = prepare_dataset(\n",
    "        train_sentences, char_to_id, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    dev_data = prepare_dataset(\n",
    "        dev_sentences, char_to_id, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    test_data = prepare_dataset(\n",
    "        test_sentences, char_to_id, tag_to_id, FLAGS.lower\n",
    "    )\n",
    "    print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "        len(train_data), len(dev_data), len(test_data)))\n",
    "\n",
    "    train_manager = BatchManager(train_data, FLAGS.batch_size)\n",
    "    dev_manager = BatchManager(dev_data, 100)\n",
    "    test_manager = BatchManager(test_data, 100)\n",
    "    # make path for store log and model if not exist\n",
    "    make_path(FLAGS)\n",
    "    config = config_model(char_to_id, tag_to_id)\n",
    "\n",
    "    log_path = os.path.join(FLAGS.log_file, \"log\")\n",
    "    logger = get_logger(log_path)\n",
    "    print_config(config, logger)\n",
    "\n",
    "    # limit GPU memory\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    steps_per_epoch = train_manager.len_data\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        model = create_model(sess, Model, FLAGS.ckpt_path, load_word2vec, config, id_to_char, logger)\n",
    "        logger.info(\"start training\")\n",
    "        loss = []\n",
    "        for i in range(100):\n",
    "            for batch in train_manager.iter_batch(shuffle=True):\n",
    "                step, batch_loss = model.run_step(sess, True, batch)\n",
    "                loss.append(batch_loss)\n",
    "                if step % FLAGS.steps_check == 0:\n",
    "                    iteration = step // steps_per_epoch + 1\n",
    "                    logger.info(\"iteration:{} step:{}/{}, \"\n",
    "                                \"NER loss:{:>9.6f}\".format(\n",
    "                        iteration, step % steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "                    loss = []\n",
    "\n",
    "            best = evaluate(sess, model, \"dev\", dev_manager, id_to_tag, logger)\n",
    "            if best:\n",
    "                save_model(sess, model, FLAGS.ckpt_path, logger)\n",
    "            evaluate(sess, model, \"test\", test_manager, id_to_tag, logger)\n",
    "\n",
    "\n",
    "def evaluate_line():\n",
    "    config = load_config(FLAGS.config_file)\n",
    "    logger = get_logger(FLAGS.log_file)\n",
    "    # limit GPU memory\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    with open(FLAGS.map_file, \"rb\") as f:\n",
    "        char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        model = create_model(sess, Model, FLAGS.ckpt_path, load_word2vec, config, id_to_char, logger)\n",
    "        while True:\n",
    "            line = input(\"请输入测试句子:\")\n",
    "            result = model.evaluate_line(sess, input_from_line(line, char_to_id), id_to_tag)\n",
    "            print(result)\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}